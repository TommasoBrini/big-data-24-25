{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 103 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:24:23.759474Z",
     "start_time": "2024-11-22T16:23:53.172722Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-PSTRJPQO:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1732292652028)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f504e515fbb5fa1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T12:29:34.957451Z",
     "start_time": "2024-11-21T12:29:34.284807Z"
    }
   },
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@6ed06efe\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-22T16:24:36.698224Z",
     "start_time": "2024-11-22T16:24:34.284290Z"
    }
   },
   "source": [
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row:String) = {\n",
    "    val usaf = row.substring(4,10)\n",
    "    val wban = row.substring(10,15)\n",
    "    val year = row.substring(15,19)\n",
    "    val month = row.substring(19,21)\n",
    "    val day = row.substring(21,23)\n",
    "    val airTemperature = row.substring(87,92)\n",
    "    val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "    (usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "}\n",
    "\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "        if (str.isEmpty)\n",
    "            return 0\n",
    "        else\n",
    "            return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    (columns(0),columns(1),columns(2),columns(3),columns(4),latitude,longitude,elevation,columns(9),columns(10))  \n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseWeather: (row: String)(String, String, String, String, String, Int, Boolean)\r\n",
       "parseStation: (row: String)(String, String, String, String, String, Double, Double, Double, String, String)\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:24:43.745230Z",
     "start_time": "2024-11-22T16:24:39.272814Z"
    }
   },
   "source": [
    "val rddWeather = sc.\n",
    "  textFile(\"../../../../datasets/big/weather-sample10.txt\").\n",
    "  map(x => parseWeather(x))\n",
    "val rddStation = sc.\n",
    "  textFile(\"../../../../datasets/weather-stations.csv\").\n",
    "  map(x => parseStation(x))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddWeather: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[2] at map at <console>:29\r\n",
       "rddStation: org.apache.spark.rdd.RDD[(String, String, String, String, String, Double, Double, Double, String, String)] = MapPartitionsRDD[5] at map at <console>:32\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 103-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "  - To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```val p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-22T16:25:01.078399Z",
     "start_time": "2024-11-22T16:24:46.543419Z"
    }
   },
   "source": [
    "// Average temperature for every month\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "  map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, Double)] = Array((10,13.32), (11,8.15), (12,4.08), (01,3.06), (02,5.5), (03,8.31), (04,11.75), (05,15.83), (06,18.53), (07,19.96), (08,20.31), (09,17.24))\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:25:11.675137Z",
     "start_time": "2024-11-22T16:25:02.919504Z"
    }
   },
   "source": [
    "// Maximum temperature for every month\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(String, Int)] = Array((10,55), (11,43), (12,47), (01,55), (02,47), (03,44), (04,48), (05,49), (06,56), (07,56), (08,56), (09,55))\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exercise 1 => cached repetition operations",
   "id": "69d26f21938b347d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:25:34.851987Z",
     "start_time": "2024-11-22T16:25:18.343925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val partitioner = new HashPartitioner(8)\n",
    "val cachedRddWeather = rddWeather.\n",
    "  filter(_._6 < 999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  partitionBy(partitioner).\n",
    "  cache()\n",
    "\n",
    "// Average temperature for every month\n",
    "cachedRddWeather.aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "  map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "  collect()\n",
    "\n",
    "// Maximum temperature for every month\n",
    "cachedRddWeather.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "id": "44ab27f1e5b79bb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "cachedRddWeather: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[15] at partitionBy at <console>:30\r\n",
       "res2: Array[(String, Int)] = Array((11,43), (08,56), (09,55), (12,47), (01,55), (02,47), (03,44), (04,48), (05,49), (06,56), (07,56), (10,55))\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 103-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-22T16:25:53.133116Z",
     "start_time": "2024-11-22T16:25:50.087784Z"
    }
   },
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p2 = new HashPartitioner(8)\n",
    "\n",
    "// _1 and _2 are the fields composing the key; _4 and _8 are country and elevation, respectively\n",
    "val rddS1 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  cache().\n",
    "  map({case (k,v) => (k,(v._4,v._8))})\n",
    "val rddS2 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache().\n",
    "  partitionBy(p2)\n",
    "val rddS3 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache()\n",
    "val rddS4 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "val rddS5 = rddStation.\n",
    "  map(x => (x._1 + x._2, (x._4,x._8))).\n",
    "  partitionBy(p2).\n",
    "  cache()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "p2: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rddS1: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[21] at map at <console>:34\r\n",
       "rddS2: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[24] at partitionBy at <console>:39\r\n",
       "rddS3: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[27] at map at <console>:43\r\n",
       "rddS4: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[30] at partitionBy at <console>:48\r\n",
       "rddS5: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[32] at partitionBy at <console>:52\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Soluzione => keyBy() e map() rompono la partizione, quindi devono essere fatte prima di partitionBy(). Tutto ciò che accade dopo cache() non è salvato e deve essere ricalcolato ogni volta, quindi mettere cache() il più tardi possibile.\n",
    "\n",
    "====>>>> rddS4 e rddS5 sono le opzioni migliori"
   ],
   "id": "6b09257e30022c92"
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 103-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:26:56.108144Z",
     "start_time": "2024-11-22T16:26:54.119530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// Clear the cache\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())"
   ],
   "id": "60c0045895fdc726",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:28:06.672720Z",
     "start_time": "2024-11-22T16:28:06.230736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val p3 = new HashPartitioner(8)\n",
    "\n",
    "val rddSt = rddStation.\n",
    "  map( r => (r._1 + r._2, (r._3, r._4))).\n",
    "  partitionBy(p3)\n",
    "val rddWe = rddWeather.\n",
    "  filter(_._6 < 999).\n",
    "  map (r => (r._1 + r._2, r._6)).\n",
    "  partitionBy(p3)\n",
    "\n",
    "// JOIN\n",
    "val rddJoined = rddSt.join(rddWe).cache()\n"
   ],
   "id": "29e07b7873c646d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p3: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rddSt: org.apache.spark.rdd.RDD[(String, (String, String))] = ShuffledRDD[34] at partitionBy at <console>:33\r\n",
       "rddWe: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[37] at partitionBy at <console>:37\r\n",
       "rddJoined: org.apache.spark.rdd.RDD[(String, ((String, String), Int))] = MapPartitionsRDD[40] at join at <console>:40\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:28:32.883151Z",
     "start_time": "2024-11-22T16:28:12.325781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//MAXIMUM TEMPERATURE FOR EVERY CITY\n",
    "rddJoined.\n",
    "  map(r => (r._2._1._1, r._2._2)).\n",
    "  reduceByKey((x,y) => {if(x>y) x else y})\n",
    "  .collect()"
   ],
   "id": "6de16799ad20f4fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(String, Int)] = Array((RYBINSK,28), (POWRANNA (TASMANIA FEEDLOT),31), (GUNNEDAH POOL,33), (HUICHON,34), (NJURBA,40), (RICHMOND OPERATION CENTRE,7), (TURI,23), (KARSHI,39), (ENVIRONM BUOY 62425,-10), (ILIAMNA AIRPORT,20), (SINAIA-1500,26), (UFA,36), (DIKILI,31), (HART ISLAND (AUT)  NS,22), (CAHOKIA/ST. LOUIS,36), (BOJNORD,36), (RONG-SHUI,25), (ONEGA,29), (ANJU,32), (GANDER INTL,29), (KRUSEVAC,40), (LES ESCALDES,19), (UST-KARENGA,0), (HINOJOSA DEL DUQUE,26), (OBAN,22), (MAREEBA AIRPORT,32), (FEIRA DE SANTANA,21), (PLATFORM NO. 62125,21), (SCHOOLCRAFT CO,27), (EL BORMA,45), (LUSAKA INTL,35), (SURABAYA/PERAK,33), (PEMBERTON AIRPORT  BC,34), (ALLENDALE,35), (TED STEVENS ANCHORAGE INTL,21), (HAMILTON/RAVALLI CO,33), (ATHENS/BEN EPPS AIRPORT,37), (GLEBA CELESTE,36), (SURABAYA/GEDA...\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:29:53.981275Z",
     "start_time": "2024-11-22T16:29:51.663815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// The maximum temperature for every city in the UK\n",
    "rddJoined.\n",
    "  filter(r => r._2._1._2 == \"UK\").\n",
    "  map(r => (r._2._1._1, r._2._2)).\n",
    "  reduceByKey((x,y) => {if(x>y) x else y})\n",
    "  .collect()"
   ],
   "id": "5d7c48b63bb70ee7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(String, Int)] = Array((WATTISHAM,30), (CAMBRIDGE,30), (NORWICH,31), (PRESTWICK RNAS,21), (LYNEHAM,28), (WAINFLEET (AUT),24), (GREAT MALVERN,28), (TAIN RANGE (SAWS),21), (COVENTRY,28), (LOSSIEMOUTH,23), (LEEDS WEATHER CTR,30), (SEA LION ISLAND,12), (ISLE OF PORTLAND,20), (ENVIRONM BUOY 62128,17), (OBAN,22), (PEMBRY SANDS,23), (NEWCASTLE,23), (GLASGOW,26), (HONINGTON,26), (PERSHORE,26), (ST MAWGAN,23), (BARKSTON HEATH,13), (MADLEY,23), (BALTASOUND NO.2,17), (BOLTSHOPE PARK,20), (SELLA NESS,17), (ODIHAM,27), (LOCHRANZA NO3,21), (KIRKWALL,19), (ASPATRIA,25), (FOULA,13), (LAKENHEATH,33), (SULE SKERRY,16), (SENNYBRIDGE NO2,26), (CULDROSE,22), (GREENOCK MRCC,24), (NOTTINGHAM/WATNALL,29), (BUTT OF LEWIS (LH),8), (GRAVESEND-BROADNESS,27), (SAUGHALL,21), (LEEMING,28), (PLATFORM NO. 6...\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:30:14.169321Z",
     "start_time": "2024-11-22T16:29:55.355780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//Sort the results by descending temperature\n",
    "rddJoined.\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  collect()\n"
   ],
   "id": "2eb681ae5fc5aa3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(String, ((String, String), Int))] = Array((25400099999,((ZYRYANKA,RS),31)), (25400099999,((ZYRYANKA,RS),28)), (25400099999,((ZYRYANKA,RS),27)), (25400099999,((ZYRYANKA,RS),27)), (25400099999,((ZYRYANKA,RS),27)), (25400099999,((ZYRYANKA,RS),24)), (25400099999,((ZYRYANKA,RS),22)), (25400099999,((ZYRYANKA,RS),22)), (25400099999,((ZYRYANKA,RS),21)), (25400099999,((ZYRYANKA,RS),21)), (25400099999,((ZYRYANKA,RS),21)), (25400099999,((ZYRYANKA,RS),20)), (25400099999,((ZYRYANKA,RS),20)), (25400099999,((ZYRYANKA,RS),19)), (25400099999,((ZYRYANKA,RS),19)), (25400099999,((ZYRYANKA,RS),19)), (25400099999,((ZYRYANKA,RS),19)), (25400099999,((ZYRYANKA,RS),19)), (25400099999,((ZYRYANKA,RS),18)), (25400099999,((ZYRYANKA,RS),18)), (25400099999,((ZYRYANKA,RS),18)), (25400099999,((ZYRYANKA,RS),...\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 103-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs."
   ]
  },
  {
   "cell_type": "code",
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-22T16:30:22.330309Z",
     "start_time": "2024-11-22T16:30:20.100511Z"
    }
   },
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val memRdd = rddWeather.cache()\n",
    "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
    "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel._\r\n",
       "memRdd: rddWeather.type = MapPartitionsRDD[2] at map at <console>:29\r\n",
       "memSerRdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[51] at map at <console>:34\r\n",
       "diskRdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[52] at map at <console>:35\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:34:04.465485Z",
     "start_time": "2024-11-22T16:33:17.055784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memRdd.count()\n",
    "memSerRdd.count()\n",
    "diskRdd.count()\n",
    "// per salvare e visualizzare nello storage, bisogna fare un'operazione di tipo actions (collect, count ecc..), perchè le transformations (map ecc..) sono lazy\n",
    "// memSerRdd pesa meno della metà di memSer, perchè serializza\n",
    "// diskRdd pesa come memSerRdd (ma tutto sul disco), perchè sul disco è sempre serializzato"
   ],
   "id": "1a77803d3181e9ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 4987830\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d",
   "metadata": {
    "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d"
   },
   "source": [
    "## 103-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
   "metadata": {
    "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-22T16:43:43.096998Z",
     "start_time": "2024-11-22T16:41:18.501508Z"
    }
   },
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val pa = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddWea = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  persist()\n",
    "val rddSta = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(pa).\n",
    "  cache()\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddWea.collect()\n",
    "rddSta.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "pa: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rddWea: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Int, Boolean))] = MapPartitionsRDD[54] at keyBy at <console>:39\r\n",
       "rddSta: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = ShuffledRDD[56] at partitionBy at <console>:43\r\n",
       "res8: Array[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = Array((00701199999,(007011,99999,CWOS 07011,\"\",\"\",0.0,0.0,0.0,20120101,20121129)), (00704499999,(007044,99999,CWOS 07044,\"\",\"\",0.0,0.0,0.0,20120127,20120127)), (00840599999,(008405,99999,XM14,\"\",\"\",0.0,0.0,0.0,20120101,20120827)), (00841699999,(008416,...\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "30a6822816cd65d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:48:40.510473Z",
     "start_time": "2024-11-22T16:45:39.656803Z"
    }
   },
   "source": [
    "// Is it better to simply join the two RDDs..\n",
    "rddWea.\n",
    "  join(rddSta).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[(String, Int)] = Array((RYBINSK,28), (POWRANNA (TASMANIA FEEDLOT),31), (GUNNEDAH POOL,33), (HUICHON,34), (NJURBA,40), (RICHMOND OPERATION CENTRE,7), (TURI,23), (KARSHI,39), (ENVIRONM BUOY 62425,-10), (ILIAMNA AIRPORT,20), (SINAIA-1500,26), (UFA,36), (DIKILI,31), (HART ISLAND (AUT)  NS,22), (CAHOKIA/ST. LOUIS,36), (BOJNORD,36), (RONG-SHUI,25), (ONEGA,29), (ANJU,32), (GANDER INTL,29), (KRUSEVAC,40), (LES ESCALDES,19), (UST-KARENGA,0), (HINOJOSA DEL DUQUE,26), (OBAN,22), (MAREEBA AIRPORT,32), (FEIRA DE SANTANA,21), (PLATFORM NO. 62125,21), (SCHOOLCRAFT CO,27), (EL BORMA,45), (LUSAKA INTL,35), (SURABAYA/PERAK,33), (PEMBERTON AIRPORT  BC,34), (ALLENDALE,35), (TED STEVENS ANCHORAGE INTL,21), (HAMILTON/RAVALLI CO,33), (ATHENS/BEN EPPS AIRPORT,37), (GLEBA CELESTE,36), (SURABAYA/GEDA...\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "4e0e5f9827be45d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:52:58.912814Z",
     "start_time": "2024-11-22T16:49:40.898666Z"
    }
   },
   "source": [
    "// ..to enforce on rddW1 the same partitioner of rddS..\n",
    "rddWea.\n",
    "  partitionBy(pa).\n",
    "  join(rddSta).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[(String, Int)] = Array((RYBINSK,28), (POWRANNA (TASMANIA FEEDLOT),31), (GUNNEDAH POOL,33), (HUICHON,34), (NJURBA,40), (RICHMOND OPERATION CENTRE,7), (TURI,23), (KARSHI,39), (ENVIRONM BUOY 62425,-10), (ILIAMNA AIRPORT,20), (SINAIA-1500,26), (UFA,36), (DIKILI,31), (HART ISLAND (AUT)  NS,22), (CAHOKIA/ST. LOUIS,36), (BOJNORD,36), (RONG-SHUI,25), (ONEGA,29), (ANJU,32), (GANDER INTL,29), (KRUSEVAC,40), (LES ESCALDES,19), (UST-KARENGA,0), (HINOJOSA DEL DUQUE,26), (OBAN,22), (MAREEBA AIRPORT,32), (FEIRA DE SANTANA,21), (PLATFORM NO. 62125,21), (SCHOOLCRAFT CO,27), (EL BORMA,45), (LUSAKA INTL,35), (SURABAYA/PERAK,33), (PEMBERTON AIRPORT  BC,34), (ALLENDALE,35), (TED STEVENS ANCHORAGE INTL,21), (HAMILTON/RAVALLI CO,33), (ATHENS/BEN EPPS AIRPORT,37), (GLEBA CELESTE,36), (SURABAYA/GED...\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "d50b618652ac67fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:54:34.280659Z",
     "start_time": "2024-11-22T16:54:29.029188Z"
    }
   },
   "source": [
    "// ..or to exploit broadcast variables?\n",
    "val bRddS = sc.broadcast(rddSta.map(x => (x._1, x._2._3)).collectAsMap())\n",
    "val rddJ = rddWea.\n",
    "  map({case (k,v) => (bRddS.value.get(k),v._6)}).\n",
    "  filter(_._1!=None)\n",
    "rddJ.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bRddS: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,String]] = Broadcast(31)\r\n",
       "rddJ: org.apache.spark.rdd.RDD[(Option[String], Int)] = MapPartitionsRDD[70] at filter at <console>:38\r\n",
       "res11: Array[(Option[String], Int)] = Array((Some(WHYALLA),39), (Some(ODATE NOSHIRO),34), (Some(GAJNY),30), (Some(CRESTON MUNI),36), (Some(FLAG ISLAND),26), (Some(SOUTHEND  SASK),27), (Some(AULTBEA NO2),22), (Some(HANOVER CO MUNI),33), (Some(FORT WORTH NAVAL AIR STATION JRB/CARSWELL FIELD),40), (Some(SARAJEVO),36), (Some(ENNADAI LAKE (AUT)  NU),27), (Some(MAMOU),31), (Some(CHUPUNGNYEONG),32), (Some(BACHELORS ISLAND MARINE  MAN),30), (Some(HAMILTON/RAVALLI CO),33), (Some(LA PLATA),31), (Some(ANQING),35), (Some(RENMARK),40), (Some(HEIDRUN),16), (Some(LUIS MUNOZ MARIN IN),32), (Some(HARTFORD...\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confrontando i tempi, la terza è la soluzione migliore",
   "id": "4b2a702dac0beff0"
  },
  {
   "cell_type": "markdown",
   "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33",
   "metadata": {
    "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33"
   },
   "source": [
    "## 103-6 Optimizing Exercise 3\n",
    "\n",
    "Start from the result of the last job of Exercise 3; is there a more efficient way to compute the same result?\n",
    "- Try it on weather-sample10\n",
    "- Hint: consider that each station is located in only one country"
   ]
  },
  {
   "cell_type": "code",
   "id": "47748353-fb4b-432f-af79-d1136453b956",
   "metadata": {
    "id": "47748353-fb4b-432f-af79-d1136453b956",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-22T17:03:52.132352Z",
     "start_time": "2024-11-22T17:00:33.999823Z"
    }
   },
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "import org.apache.spark.storage.StorageLevel._\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rddS: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = ShuffledRDD[73] at partitionBy at <console>:41\r\n",
       "rddW: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Int, Boolean))] = ShuffledRDD[76] at partitionBy at <console>:46\r\n",
       "res12: Array[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = Array((00701199999,(007011,99999,CWOS 07011,\"\",\"\",0.0,0.0,0.0,20120101,20121129)), (00704499999,(007044,99999,CWOS 07044,\"\",\"\",0.0,0.0,0.0,20120127,20120127)), (00840599999,(008405,99999,XM14,\"\",\"\",0.0,0.0,0...\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
   "metadata": {
    "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-22T17:05:24.126574Z",
     "start_time": "2024-11-22T17:04:07.682503Z"
    }
   },
   "source": [
    "// First version\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  filter(_._2._2._4==\"UK\").\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[(Int, String)] = Array((40,PLATFORM 62120), (33,LAKENHEATH), (31,NORWICH), (31,HEATHROW), (31,MILDENHALL), (31,MARHAM), (30,WATTISHAM), (30,CAMBRIDGE), (30,LEEDS WEATHER CTR), (30,LONDON WEA CENTER), (30,COSFORD), (30,ANDREWSFIELD), (30,DURHAM TEES VALLEY AIRPORT), (30,CRANFIELD), (30,HUMBERSIDE), (30,NOTTINGHAM EAST MIDLANDS), (30,COLTISHALL), (30,BENSON), (30,SHEFFIELD CITY), (29,NOTTINGHAM/WATNALL), (29,NORTHOLT), (29,CONINGSBY), (29,HEMSBY), (29,LINTON ON OUSE), (29,CITY), (29,CHURCH LAWFORD), (29,GLOUCESTERSHIRE), (29,BIRMINGHAM), (29,BRIZE NORTON), (29,HAWARDEN), (29,SOUTHEND), (29,COTTESMORE), (28,LYNEHAM), (28,GREAT MALVERN), (28,COVENTRY), (28,LEEMING), (28,CAPEL CURIG NO3), (28,CRANWELL), (28,LIVERPOOL), (28,LUTON), (28,CROSBY), (28,WADDINGTON), (28,NEWTON), (28,L...\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T17:06:42.496398Z",
     "start_time": "2024-11-22T17:05:29.921972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// fare la filter prima della join, in modo da ridurre la dimensione della join()\n",
    "val rddSFiltered = rddS.filter(_._2._4 == \"UK\")\n",
    "\n",
    "rddW.\n",
    "  join(rddSFiltered).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()\n"
   ],
   "id": "18df536aff411910",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddSFiltered: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = MapPartitionsRDD[87] at filter at <console>:39\r\n",
       "res14: Array[(Int, String)] = Array((40,PLATFORM 62120), (33,LAKENHEATH), (31,NORWICH), (31,HEATHROW), (31,MILDENHALL), (31,MARHAM), (30,WATTISHAM), (30,CAMBRIDGE), (30,LEEDS WEATHER CTR), (30,LONDON WEA CENTER), (30,COSFORD), (30,ANDREWSFIELD), (30,DURHAM TEES VALLEY AIRPORT), (30,CRANFIELD), (30,HUMBERSIDE), (30,NOTTINGHAM EAST MIDLANDS), (30,COLTISHALL), (30,BENSON), (30,SHEFFIELD CITY), (29,NOTTINGHAM/WATNALL), (29,NORTHOLT), (29,CONINGSBY), (29,HEMSBY), (29,LINTON ON OUSE), (29,CITY), (29,CHURCH LAWFORD), (29,GLOUCESTERSHIRE), (29,BIRMINGHAM), (29,BRIZE NORTON), (29,HAWARDEN), (29,SOUTHEND), (29,COTTESMO...\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
